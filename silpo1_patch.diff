*** Begin Patch
*** Update File: src/silpo/config.py
@@
 class Settings:
@@
-    max_pages: int = int(os.getenv("SILPO_MAX_PAGES", "10"))
-    timeout_ms: int = int(os.getenv("SILPO_TIMEOUT_MS", "45000"))
+    max_pages: int = int(os.getenv("SILPO_MAX_PAGES", "10"))
+    timeout_ms: int = int(os.getenv("SILPO_TIMEOUT_MS", "45000"))
+
+    # Whether to fallback to HTML scraping when the API returns no products.
+    use_html_fallback: bool = os.getenv("SILPO_USE_HTML_FALLBACK", "true").lower() in {"1", "true", "yes"}
+
+    # Whether to use the alternative catalog API (api.catalog.ecom.silpo.ua) when
+    # the GetCategoryProducts endpoint is not captured.  The alternative API
+    # requires a POST body specifying the category ID and pagination.  See
+    # README for details.
+    use_alt_api: bool = os.getenv("SILPO_USE_ALT_API", "true").lower() in {"1", "true", "yes"}
*** End Patch
*** Update File: src/silpo/api_discovery.py
@@
-def discover_get_category_products_template(
+from .config import settings
+
+def discover_get_category_products_template(
@@ def discover_get_category_products_template(
-        if not captured:
-            raise RuntimeError(
-                "API template not captured: GetCategoryProducts не спіймано після wait/scroll. "
-                "Ймовірно Silpo блокує headless або змінив механіку каталогу."
-            )
+        if not captured:
+            # If the GetCategoryProducts endpoint is not captured and the alternative API
+            # fallback is enabled, build an ApiTemplate for the alternative endpoint.
+            # See README for details on the POST body schema.  We default to
+            # category ID 234, page size 24.  Pagination is controlled by the caller.
+            if settings.use_alt_api:
+                body = {
+                    "query": {"collection": "EcomCatalogGlobal"},
+                    "filter": {"category": [234]},
+                    "page": {"size": 24, "number": 1},
+                }
+                return ApiTemplate(
+                    endpoint="https://api.catalog.ecom.silpo.ua/api/2.0/exec/EcomCatalogGlobal",
+                    method="POST",
+                    headers={"content-type": "application/json", "accept": "application/json"},
+                    cookies={},
+                    body=body,
+                )
+            raise RuntimeError(
+                "API template not captured: GetCategoryProducts не спіймано після wait/scroll. "
+                "Ймовірно Silpo блокує headless або змінив механіку каталогу."
+            )
*** End Patch
*** Add File: src/silpo/html_scraper.py
+"""HTML fallback scraping for silpo.ua.
+
+This module provides helper functions to extract product information from the
+client‑rendered HTML when the API returns no products.  It uses
+BeautifulSoup to parse the HTML and searches for anchor tags linking to
+product pages (href starting with "/product/").  The product title and price
+information are extracted from the text of the anchor.  Brand, product type,
+fat percentage, pack size, and price per unit are derived using the
+existing extractors from the package.
+
+Note: This fallback scraping is a best‑effort approach.  It may not return
+all products if the HTML structure changes dramatically.  It is intended as
+a backup when the API is unavailable or returns zero items.
+"""
+
+from __future__ import annotations
+
+import re
+from typing import List, Optional, Tuple
+
+import requests
+from bs4 import BeautifulSoup  # type: ignore
+
+from .extractors import (
+    extract_brand,
+    extract_product_type,
+    extract_fat_pct,
+    extract_pack,
+    compute_price_per_unit,
+    to_float,
+)
+from .model import ProductRow
+
+
+PRICE_RE = re.compile(r"(\d{1,4}(?:[.,]\d{2})?)\s*грн", re.IGNORECASE)
+DISCOUNT_RE = re.compile(r"-\s*(\d{1,2})\s*%", re.IGNORECASE)
+
+
+def fetch_html_page(url: str, user_agent: str) -> Optional[str]:
+    """Fetch a page with custom User‑Agent and return its HTML text.
+
+    Returns None on HTTP error or network failure.
+    """
+    try:
+        resp = requests.get(
+            url,
+            headers={
+                "User-Agent": user_agent,
+                "Accept-Language": "uk-UA,uk;q=0.9,en;q=0.8",
+                "Cache-Control": "no-cache",
+                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
+            },
+            timeout=30,
+        )
+        if resp.status_code != 200:
+            return None
+        return resp.text
+    except Exception:
+        return None
+
+
+def _parse_price_block(text: str) -> Optional[Tuple[float, Optional[float], Optional[str], str]]:
+    """Parse price information from a product text block.
+
+    Returns (current_price, old_price, discount_pct, price_type) or None.
+    """
+    prices = [to_float(m) for m in PRICE_RE.findall(text)]
+    if not prices:
+        return None
+    current = prices[0]
+    old: Optional[float] = prices[1] if len(prices) >= 2 else None
+    discount_match = DISCOUNT_RE.search(text)
+    discount_pct = discount_match.group(1) if discount_match else None
+    price_type = "discount" if discount_pct else "regular"
+    return current, old, discount_pct, price_type
+
+
+def extract_products_from_html(
+    html: str,
+    page_url: str,
+    page_number: int,
+    batch_stamp: str,
+    user_agent: str,
+) -> List[ProductRow]:
+    """Extract product rows from the HTML of a category page.
+
+    This function searches for anchors linking to product pages and infers
+    product properties from the anchor text.
+    """
+    soup = BeautifulSoup(html, "html.parser")
+    rows: List[ProductRow] = []
+    anchors = soup.find_all("a", href=True)
+    seen: set[str] = set()
+    for a in anchors:
+        href = a.get("href", "")
+        if not href.startswith("/product/"):
+            continue
+        text = a.get_text(" ", strip=True)
+        if not text or "грн" not in text:
+            continue
+        key = href + "::" + text[:100]
+        if key in seen:
+            continue
+        seen.add(key)
+        price_info = _parse_price_block(text)
+        if not price_info:
+            continue
+        current, old, discount_pct, price_type = price_info
+        if current is None:
+            continue
+        title = re.sub(r"\s+", " ", text).strip()
+        if len(title) < 5:
+            continue
+        brand = extract_brand(title)
+        ptype = extract_product_type(title)
+        fat = extract_fat_pct(title)
+        pack = extract_pack(title)
+        per_unit = compute_price_per_unit(current, pack)
+        rating = ""
+        product_url = f"https://silpo.ua{href}"
+        rows.append(
+            ProductRow(
+                upload_ts=batch_stamp,
+                page_url=page_url,
+                page_number=page_number,
+                source="https://silpo.ua",
+                product_title=title,
+                brand=brand,
+                product_type=ptype,
+                fat_pct=fat,
+                pack_qty=pack.qty,
+                pack_unit=pack.unit,
+                price_current=current,
+                price_old=old if old is not None else "",
+                discount_pct=discount_pct or "",
+                price_per_l_or_kg_or_piece=per_unit,
+                rating=rating,
+                price_type=price_type,
+            )
+        )
+    return rows
*** End Patch
*** Update File: src/silpo/scraper.py
@@
-from .model import ProductRow
+from .model import ProductRow
+from .html_scraper import extract_products_from_html, fetch_html_page
+from .config import settings
@@ def scrape_pages_via_api(
-        resp = sess.request(
-            method=template.method,
-            url=template.endpoint,
-            headers=template.headers,
-            cookies=template.cookies,
-            json=body,
-            timeout=45,
-        )
-
-        if resp.status_code != 200:
-            raise RuntimeError(f"API HTTP {resp.status_code} page={page_num}: {resp.text[:300]}")
-
-        data = resp.json()
-        (debug_dir / f"silpo_api_page_{page_num}.json").write_text(
-            json.dumps(data, ensure_ascii=False, indent=2),
-            encoding="utf-8",
-        )
-
-        products = _find_product_list(data)
-        if not products:
-            raise RuntimeError(f"Products list not found in JSON page={page_num}. See debug JSON.")
-
-        page_url = category_url if page_num == 1 else f"{category_url}?page={page_num}"
-
-        for p in products:
-            title = _pick_title(p)
-            if not title:
-                continue
-
-            current, old, discount_pct, price_type = _extract_prices(p)
-            if current is None:
-                continue
-
-            brand = extract_brand(title)
-            ptype = extract_product_type(title)
-            fat = extract_fat_pct(title)
-            pack = extract_pack(title)
-            per_unit = compute_price_per_unit(current, pack)
-            rating = _pick_rating(p)
-
-            out.append(ProductRow(
-                upload_ts=batch,
-                page_url=page_url,
-                page_number=page_num,
-                source="https://silpo.ua",
-                product_title=title,
-                brand=brand,
-                product_type=ptype,
-                fat_pct=fat,
-                pack_qty=pack.qty,
-                pack_unit=pack.unit,
-                price_current=float(current),
-                price_old=(float(old) if old is not None else ""),
-                discount_pct=discount_pct,
-                price_per_l_or_kg_or_piece=per_unit,
-                rating=rating,
-                price_type=price_type,
-            ))
+        # Attempt to call the API.  If it fails or returns no products and
+        # HTML fallback is enabled, fall back to HTML parsing.
+        products: List[Dict[str, Any]] = []
+        page_url = category_url if page_num == 1 else f"{category_url}?page={page_num}"
+        try:
+            resp = sess.request(
+                method=template.method,
+                url=template.endpoint,
+                headers=template.headers,
+                cookies=template.cookies,
+                json=_set_pagination(template.body, page_num),
+                timeout=45,
+            )
+            if resp.status_code == 200:
+                data = resp.json()
+                (debug_dir / f"silpo_api_page_{page_num}.json").write_text(
+                    json.dumps(data, ensure_ascii=False, indent=2),
+                    encoding="utf-8",
+                )
+                products = _find_product_list(data)
+        except Exception:
+            products = []
+
+        # If API returned no products and HTML fallback is enabled, parse HTML
+        if not products and settings.use_html_fallback:
+            html = fetch_html_page(page_url, template.headers.get("user-agent", ""))
+            if html:
+                rows = extract_products_from_html(html, page_url, page_num, batch, template.headers.get("user-agent", ""))
+                out.extend(rows)
+                continue
+
+        if not products:
+            raise RuntimeError(f"Products list not found in API JSON or HTML page={page_num}")
+
+        for p in products:
+            title = _pick_title(p)
+            if not title:
+                continue
+
+            current, old, discount_pct, price_type = _extract_prices(p)
+            if current is None:
+                continue
+
+            brand = extract_brand(title)
+            ptype = extract_product_type(title)
+            fat = extract_fat_pct(title)
+            pack = extract_pack(title)
+            per_unit = compute_price_per_unit(current, pack)
+            rating = _pick_rating(p)
+
+            out.append(ProductRow(
+                upload_ts=batch,
+                page_url=page_url,
+                page_number=page_num,
+                source="https://silpo.ua",
+                product_title=title,
+                brand=brand,
+                product_type=ptype,
+                fat_pct=fat,
+                pack_qty=pack.qty,
+                pack_unit=pack.unit,
+                price_current=float(current),
+                price_old=(float(old) if old is not None else ""),
+                discount_pct=discount_pct,
+                price_per_l_or_kg_or_piece=per_unit,
+                rating=rating,
+                price_type=price_type,
+            ))
*** End Patch
*** End Patch
